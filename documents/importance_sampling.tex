\documentclass[12pt]{article}

% typesetting rules
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\thetractor}{\project{The~Tractor}}

% all symbols defined here; use lexigraphic names in text
\newcommand{\given}{\,|\,}
\newcommand{\data}{D}
\newcommand{\shear}{\gamma}
\newcommand{\ellip}{\epsilon}
\newcommand{\intrinsic}{\ellip^{\mathrm{int}}}
\newcommand{\prior}{I}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\nspars}{\omega}
\newcommand{\hyperpars}{\alpha}
\newcommand{\setof}[1]{\left\{{#1}\right\}}

\begin{document}\sloppy\sloppypar

\section*{An importance-sampling approach\\ to hierarchical, probabilistic weak lensing}

\smallskip\noindent
David W. Hogg (NYU \& MPIA)\\
2014-01-24

\paragraph{abstract:}
I argue that we need to have prior PDFs or model PDFs not just over intrinsic galaxy shape,
  but also all other galaxy properties.
These PDFs are best built via hierarchical inference.
I suggest an importance-sampling approach to this hierarchical inference that may be tractable
  given our computational and team structure.

\bigskip

All information flows from the data $\data_n$ on galaxy $n$ to the local value of shear $\shear$
  by means of a likelihood function $p(\data_n\given\shear)$.
I will treat this as uncontroversial,
  and as the entire goal of what we are doing.
Of course you can reformulate $\data_n$ to include more general kinds of data
  (including imaging and spectroscopy, say, or covering a patch rather than a galaxy)
  and you can reformulate $\gamma$ to be a shear field, in two or three dimensions,
  or even a set of $\Lambda$CDM parameters that \emph{generate} the density field
  (which generates the shear field).
Also, you can treat the data $\data_n$ on galaxy $n$ to be a pixel patch
  or a catalog entry derived from that pixel patch;
  what follows here is independent of that.

One realization we (Marshall \& Hogg, and separately Schneider \& Dawson) have had is
  that this likelihood $p(\data_n\given\shear)$ involves a \emph{marginalization}
  over intrinsic shape or ellipticity $\intrinsic_n$.
That is, each galaxy delivers information about the shear $\gamma$ via its observed shape,
  and it's observed shape is a function of both the unlensed shape $\intrinsic_n$
  and the shear $\gamma$ (and possibly other things, like the point-spread function and so on).
From the point of view of weak lensing, the intrinsic shapes are uninteresting and ought to be integrated out by
\begin{eqnarray}
p(\data_n\given\shear)
  &=& \int p(\data_n\given\intrinsic_n,\shear)\,p(\intrinsic_n\given\prior)\,\dd\intrinsic_n
  \quad ,
\end{eqnarray}
  where $p(\intrinsic\given\prior)$ is the prior on intrinsic shapes,
  and $\prior$ is an object (blob) containing all our prior information.
Once again, there is enormous freedom about what one means by the shape parameter $\intrinsic_n$:
It must be the unlensed, PSF-deconvolved shape
  (the thing that would be measured in the absence of seeing and lensing),
  but in detail that shape could be represented as an ellipticity and a position angle,
  some moments or polarization parameters,
  or even a full high-resolution thumbnail image.

Importantly, even if the data $\data_n$ are considered to be a catalog entry that includes an ellipticity,
  the likelihood $p(\data_n\given\intrinsic_n,\shear)$ is not going to be trivial.
The reason is that the catalog entries are noisy and involve either PSF deconvolution or something equally ugly.
Furthermore, the parameters inside the shape object $\intrinsic_n$ are related to the data in nonlinear ways,
  so the likelihood we care about essentially \emph{cannot} be Gaussian in form,
  even for an ideally constructed catalog or imaging survey.

Another realization, which goes beyond the scope of this \documentname,
  is that you can't treat the different galaxies independently
  (as we have, by considering the $\data_n$ one at a time),
  because there has also been an implicit or explicit marginalization over point-spread function
  (and other calibration information),
  which makes the measurements non-independent, even conditioned on a fixed shear field.
This is a huge issue, which deserves treatment in a separate project.
Yet another realization, also beyond the scope,
  is that there is lensing information also in the positions, sizes, and fluxes
  of the galaxies,
  all of which we are (officially) ignoring or pretending to ignore (for now).

Now of course most shear-measuring projects do \emph{not} claim to have any prior over intrinsic shapes $\intrinsic_n$.
These projects are mistaken.  Why?
Well, either they are measuring the shear via a likelihood function $p(\data_n\given\shear)$ or they are not.
If they are not constructing a likelihood or anything that is interpretable as such,
  then (if frequentists) they aren't saturating the Cram\`er--Rao bound,
  or (if Bayesians) they aren't obeying the rules.
That is an inefficient use of telescope time and not permitted by Divine Law.
If they \emph{are} going through a likelihood function,
  then they are performing this marginalization either implicitly or explicitly
  (since the measurements do undeniably involve this intrinsic shape)
If implicitly, then they \emph{have} adopted a prior, they \emph{just don't know what that prior is}.
In particular, projects in which
  measured ellipticities of galaxies are averaged together over sky patches
  (or something equivalent)
  have made an implicit (and strong) assumption about the distribution of intrinsic shapes $\intrinsic$;
  this assumption (since unexamined) must be substantially wrong.

\section{We can't ignore non-shape parameters.}

Unfortunately, even the above discussion is too simplistic.
Any galaxy getting its shape $\intrinsic_n$ measured
  also has many other parameters that are either measured simultaneously with that shape,
  or affect the way that shape is measured,
  or are related to our prior information about what shapes we expect.
For example, in the case of \thetractor\ (Lang \& Hogg),
  the measurement of shape $\intrinsic_n$ is made simultaneously with a host of other
  non-shape parameters $\nspars_n$,
  which include position, flux, size, and S\'ersic index.
It turns out that we can't ignore these other parameters for two important reasons.

The first reason we can't ignore the non-shape parameters $\nspars$ is that they affect the likelihood.
Above, we argued that you can't have a likelihood for the shear $p(\data_n\given\shear)$ 
  without marginalizing out the intrinsic shape $\intrinsic_n$.
For the same reason, you can't have a likelihood for the shear and shape $p(\data_n\given\intrinsic_n,\shear)$
  without marginalizing out the non-shear parameters $\nspars_n$.
The reason is that any fit performed to the data will depend on these non-shape parameters,
  or that the likelihood will include covariances between the shape and non-shape parameters.
Or, technically, the likelihood will not be \emph{separable}:
\begin{eqnarray}
p(\data_n\given\nspars_n,\intrinsic_n,\shear)
  &\ne& g(\nspars_n)\,h(\intrinsic_n,\shear)
  \quad .
\end{eqnarray}
You might hope for exceptions, but I can guarantee they will not come.
Even if you could express shape estimation as purely linear fitting
  (which you cannot)
  the linear fit would itself create covariances that rule out separability here.

All this means that we are going to have to put a prior on the non-shape parameters
  and marginalize them out \emph{also}.
Furthermore, this means that responsible extant projects---%
  projects that are using efficient frequentist estimators or Bayes---%
  are \emph{already doing this marginalization} but once again implicitly.
That is, they are marginalizing out the non-shape parameters with a prior \emph{they don't know}
  (and which is therefore doomed to be inappropriate).

The second reason we can't ignore the non-shape parameters $\nspars$ is that they affect the \emph{prior}.
That is, our prior expectations on the intrinsic shape $\intrinsic_n$ of galaxy $n$
  depends on what we know about its flux, size, and S\'ersic index.
They also depend on what we know about its redshift, star-formation rate, and large-scale environment.
Indeed, we can't responsibly put a prior on $\intrinsic$ without conditioning on whatever information
  we have about the non-shape properties of the galaxy,
  whether we think of those as image-plane observables (like angular size)
  or physical properties (like star-formation rate).
That is, our priors aren't separable either:
\begin{eqnarray}
p(\nspars,\intrinsic\given\prior)
  &\ne& p(\nspars\given\prior)\,p(\intrinsic\given\prior)
\\
p(\nspars,\intrinsic\given\prior)
  &=& p(\nspars\given\prior)\,p(\intrinsic\given\nspars,\prior)
\end{eqnarray}
This is a bullet we have to bite if we are going to make best possible use of the data available;
  indeed this might be the \emph{key ingredient} missing from current cosmological weak lensing projects.

To summarize:
If we want to measure shear $\gamma$ using data $D_n$ from galaxy $n$,
  we have to marginalize out nuisance parameters.
These nuisance parameters necessarily include all the parameters $[\nspars_n,\intrinsic_n]$ describing each galaxy's
  intrinsic properties, not just its intrinsic ellipticity.

\section{Strategies}

There are three basic strategies worth considering, from easy to hard,
  with payoffs that increase with difficulty.
Unfortunately I don't have any estimate, for each strategy,
  of \emph{either} the cost \emph{or} the benefit,
  so this may be relatively useless.

The first strategy is to \emph{hard-set a prior in advance by parameterizing folk knowledge}.
The idea is to get a team of astronomers to read a good chunk of the astronomical literature,
  gather everything that is known about galaxy regularities (especially as regards shape).
For example, they will learn that high-S\'ersic galaxies tend to be rounder, and so on.
This team then writes a set of functions that sets out conditional PDFs for shape and non-shape
  parameters as a function of shape parameters,
  and an overall PDF for the top-level controlling non-shape parameters.
These functions become a prior that can be put into any measurement code
  (for example, \thetractor),
  and also used hierarchically in any full model.

The second strategy is to \emph{build an approximate empirical prior from data}.
The idea is to gather a huge set of actual maximum-likelihood galaxy measurements
  (preferably taken under similar conditions to the data we care about)
  and build an empirical PDF from these measurements using a density estimator
  (such as a KDE).
This method presumes that lensing is weak
  (so the measurements are not strongly affected by lensing),
  and that the gathered data are high in quality
  (so the maximum-likelihood estimates are good estimates).
This method is bad, because it is assembling a prior out of noisy measurements,
  but it is probably going to do a better job of creating a sensible prior
  than the team-of-experts approach.
It suffers from all the issues of density estimation,
  and there are issues of support,
  but I think these can all be handled.
(By the way, one thing you do \emph{not} want to do is use a set of posterior samples
  instead of maximum-likelihood estimates.
  Posterior samples would be worse for this task.)

The third strategy---%
  and the only probabilistically justifiable strategy in this case---%
  is to hierarchically infer the PDF (that is, the function itself) for galaxy nuisance parameters
  simultaneously with the shear parameters.
This involves building a hierarchical model with shear at the top,
  data at the bottom,
  and galaxy shape and non-shape nuisance parameters in the middle,
  plus a very flexible model on those nuisance parameters.
Probabilisitic information is generated about the shear,
  and the nuisance-parameters,
  \emph{and} the PDF those nuisance paremeters rode in on.
The nuisance parameters and their PDF can all be marginalized out.
The huge advantage of this method is that it is probabilistically righteous;
  it is (more-or-less) \emph{guaranteed} to beat all other methods.
The large disadvantages of this method are that
  it requires running \thetractor\ or equivalent low-level likelihood code
  within the inference of high-level parameters,
  it requires inference in a very flexible model
  (because the nuisance-parameter PDF must be given any flexibility it needs),
  and it does not permit treatment of the different galaxies $n$ independently:
Because all $N$ galaxies $n$ contribute to the inference of the PDF over the nuisance parameters,
  the inference and marginalization out of this PDF correlates all of the data in the inference.

That is, the best strategy looks intractable.
It may be, but there is one interesting direction to explore,
  which may permit independent inference on each galaxy but then full-sample hierarchical inference
  built on the individual-object inferences.
I will call this approach ``importance sampling''.

\section{Importance sampling}

In the importance-sampling approach (following Hogg, Myers, \& Bovy, 2010),
  the idea is to perform individual-galaxy shape inference using default or ``interim'' priors,
  and then perform the hierarchical inference by reweighting the output of the interim inferences.
The key ingredients of importance sampling are:
  interim priors that have \emph{support} everywhere that will conceivably end up with support in the final infererence,
  and individual-object samplings that are dense enough to have multiple samples in the domain of the final inference.
These two ingredients are at odds with one another:
One wants the interim priors to be broad,
  the other wants the interim priors to be narrow---%
  or else makes up for interim-prior breadth with extensive (and expensive) sampling.

For each object $n$ we generate $K$ samples $[\nspars_{nk},\intrinsic_{nk},\shear_{nk}]$,
   sampled from the interim posterior
\begin{eqnarray}
p(\nspars_n,\intrinsic_n,\shear\given\data_n)
  &=& \frac{1}{Z_n}\,p(\data_n\given\nspars_n,\intrinsic_n,\shear)\,p(\shear\given\prior_0)\,p(\nspars_n,\intrinsic_n\given\prior_0)
  \quad ,
\end{eqnarray}
  where $Z_n$ is something we will never try to compute,
  and $\prior_0$ is an object containing all the assumptions encoded in the interim priors.
Once we have this $K$-element sampling for every galaxy $n$,
  we can build importance-sampling approximations to various other marginalized likelihoods.
For example, if (in the inference) our model PDF over the nuisances $[\nspars,\intrinsic]$ has hyperparameters $\hyperpars$,
  the marginalized likelihood can be approximated by
\begin{eqnarray}
p(\data_n\given\hyperpars,\shear)
  &\approx& \frac{Z_n}{K}\,\sum_k \frac{p(\nspars_{nk},\intrinsic_{nk}\given\hyperpars)}{p(\nspars_{nk},\intrinsic_{nk}\given\prior_0)}
  \\
p(\data\given\hyperpars,\shear)
  &=& \prod_n p(\data_n\given\hyperpars,\shear)
  \\
\data
  &\equiv& \setof{\data_n}_{n=1}^N
  \quad .
\end{eqnarray}
This marginalized likelihood $p(\data\given\hyperpars,\shear)$ for the hyperparameters $\alpha$ and shear $\shear$
  is all you need for subsequent inference and marginalization.
A factor of $Z_n$ appears in each importance sampling,
  but we never need to compute the $Z_n$, because they are just irrelevant constants
  relating the data $D_n$ to the interim priors.

Once again, for the interim priors, there are two choices, given above:
We can build them using expert knowledge,
  or we can build them from empirical measurements.
I would advocate \emph{strongly} either of these over using something like ``flat in everything''
  (our current practices with \thetractor, I believe),
  because there is so much structure in the $[\nspars, \intrinsic]$ space,
  it would be a pity to be sampling useless parts of it.
That said, a huge advantage of doing ``flat in everything'' is that it is easy to code,
  and easy to use;
  the more complex interim priors are harder to code, save, communicate, and use.

As to the number $K$ of samples required to make this work,
  there is no simple answer.
One intuition that Marshall \& I share is that it might be a small number.
We propose to build a toy version of this model and approach
  to make an assessment related to setting $K$.

\end{document}
